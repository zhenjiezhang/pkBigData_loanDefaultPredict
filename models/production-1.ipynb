{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "inputFolder='../data/'\n",
    "trainFolder=inputFolder+'train/'\n",
    "testFolder=inputFolder+'test/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readFiles(type='train'):\n",
    "    if type not in ('train', 'test'):\n",
    "        print 'error: type must be either \\'train\\' or \\'test\\''\n",
    "        return\n",
    "    \n",
    "    folder=trainFolder if type=='train' else testFolder\n",
    "    \n",
    "    usersFile=folder+'user_info_'+type+'.txt'\n",
    "    banksFile=folder+'bank_detail_'+type+'.txt'\n",
    "    browseFile=folder+'browse_history_'+type+'.txt'\n",
    "    billsFile=folder+'bill_detail_'+type+'.txt'\n",
    "    loanTimeFile=folder+'loan_time_'+type+'.txt'\n",
    "    overdueFile=folder+'overdue_'+type+'.txt'\n",
    "\n",
    "    users=pd.read_csv(usersFile, \\\n",
    "              names=['id','sex','profession','education','marriage','citizen_type'],\\\n",
    "              index_col=0).sort_index()\n",
    "    banks=pd.read_csv(banksFile,\\\n",
    "              names=['user_id', 'time', 'transaction_type', 'amount', 'is_salary'])\n",
    "\n",
    "    browse=pd.read_csv(browseFile,\\\n",
    "               names=['user_id', 'time', 'browse_type', 'sub_type'])\n",
    "    bills=pd.read_csv(billsFile,\\\n",
    "              names=['user_id', 'time', 'bank_id', 'last_bill_amount', 'last_bill_pay', 'credit_line',\\\n",
    "                     'cur_bill_balance', 'cur_bill_min_due', 'transactionNum', 'cur_bill_amount', \\\n",
    "                     'adjusted_amount', 'cumulative_interest', 'available_deposit', 'available_credit', 'debt_status'])\n",
    "\n",
    "    loanTimes=pd.read_csv(loanTimeFile,\\\n",
    "              names=['user_id', 'time'], index_col=0).sort_index()\n",
    "\n",
    "    overdues=pd.read_csv(overdueFile,\\\n",
    "              names=['user_id', 'label'], index_col=0).sort_index().values.ravel() if type=='train' else None\n",
    "    \n",
    "    return users, banks, browse, bills, loanTimes, overdues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractBillFeatures(bills, index):\n",
    "    # prepare a list of banks in the bills form\n",
    "    \n",
    "    bank_ids=bills['bank_id'].unique()\n",
    "    bankBills=dict() # keys are banks ids, and values are bills associated with that bank id.\n",
    "    for b_id in bank_ids:\n",
    "        bs=bills[bills.bank_id==b_id]\n",
    "        if len(bs)>=1:\n",
    "            bankBills[b_id]=bs\n",
    "            \n",
    "    # prepare bank-wise bill features\n",
    "    billFeatures=pd.DataFrame(index, columns=['id'])\n",
    "    billFeatures.set_index(['id'], inplace=True)\n",
    "    for b_id in bankBills.keys():\n",
    "        bankBillName='bank'+str(b_id).zfill(6)\n",
    "        billFeatures[bankBillName+'AvgCreditLine']=bankBills[b_id].groupby('user_id')['credit_line'].mean()\n",
    "    #     billFeatures[bankBillName+'MaxCreditLine']=bankBills[b_id].groupby('user_id')['available_credit'].mean()\n",
    "    billFeatures.fillna(-1, inplace=True)\n",
    "    \n",
    "    return billFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractBrowseTypeFeatures(browse, index):\n",
    "    # prepare browseType wise features:\n",
    "\n",
    "    browseType_ids=browse['browse_type'].unique()\n",
    "    browseTypes=dict()\n",
    "    for b_id in browseType_ids:\n",
    "        bs=browse[browse.browse_type==b_id]\n",
    "        if len(bs)>=1:\n",
    "            browseTypes[b_id]=bs\n",
    "\n",
    "    browseTypeFeatures=pd.DataFrame(index, columns=['id'])\n",
    "    browseTypeFeatures.set_index(['id'], inplace=True)\n",
    "    for b_id in browseTypes.keys():\n",
    "        browseTypeName='browseType'+str(b_id).zfill(7)\n",
    "    #     browseTypeFeatures[browseTypeName+'TotalTimes']=browseTypes[b_id].groupby('user_id')['time'].count()\n",
    "        browseTypeFeatures[browseTypeName+'Freq']=browseTypes[b_id].groupby('user_id')['time'].count().div\\\n",
    "        (browseTypes[b_id].groupby('user_id')['time'].max()-browseTypes[b_id].groupby('user_id')['time'].min()+1+0.000001)\n",
    "    browseTypeFeatures.fillna(0, inplace=True)\n",
    "\n",
    "    return browseTypeFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainBills=pd.read_csv(trainFolder+'bill_detail_train.txt',\\\n",
    "              names=['user_id', 'time', 'bank_id', 'last_bill_amount', 'last_bill_pay', 'credit_line',\\\n",
    "                     'cur_bill_balance', 'cur_bill_min_due', 'transactionNum', 'cur_bill_amount', \\\n",
    "                     'adjusted_amount', 'cumulative_interest', 'available_deposit', 'available_credit', 'debt_status'])\n",
    "testBills=pd.read_csv(testFolder+'bill_detail_test.txt',\\\n",
    "              names=['user_id', 'time', 'bank_id', 'last_bill_amount', 'last_bill_pay', 'credit_line',\\\n",
    "                     'cur_bill_balance', 'cur_bill_min_due', 'transactionNum', 'cur_bill_amount', \\\n",
    "                     'adjusted_amount', 'cumulative_interest', 'available_deposit', 'available_credit', 'debt_status'])\n",
    "trainLoanTime=pd.read_csv(trainFolder+'loan_time_train.txt',\\\n",
    "              names=['user_id', 'time'], index_col=0).sort_index()\n",
    "testLoanTime=pd.read_csv(testFolder+'loan_time_test.txt',\\\n",
    "              names=['user_id', 'time'], index_col=0).sort_index()\n",
    "\n",
    "allBills=pd.concat([trainBills, testBills])\n",
    "loanTime=pd.concat([trainLoanTime, testLoanTime])\n",
    "\n",
    "finalTime=trainBills['time'].max()+1\n",
    "startTime=trainBills['time'].min()-1\n",
    "\n",
    "\n",
    "def afterLoanBillFreq(x):\n",
    "    lt=loanTime.loc[x['user_id'].mean(),'time']\n",
    "    return float(x[x['time']>lt]['time'].count())/float(finalTime-lt)\n",
    "\n",
    "# afterLoanBrowseFreq=afterLoanBillFreq\n",
    "\n",
    "def afterLoanSalaryChange(x):\n",
    "    lt=loanTime.loc[x['user_id'].mean(),'time']\n",
    "    beforeSalary=x[(x['is_salary']==1) & (x['time']<=lt)]['amount'].mean()\n",
    "    afterSalary=x[(x['is_salary']==1) & (x['time']>lt)]['amount'].mean()\n",
    "#     if np.isnan(afterSalary) and ~np.isnan(beforeSalary):\n",
    "#         afterSalary=0\n",
    "#     if np.isnan(beforeSalary) and ~np.isnan(afterSalary):\n",
    "#         beforeSalary=0\n",
    "    return afterSalary-beforeSalary\n",
    "\n",
    "# def afterLoanNetIncome(x):\n",
    "#     lt=trainLoanTime.loc[x['user_id'].mean(),'time']\n",
    "#     income=x[(x['transaction_type']==0) & (x['time']>lt)]['amount'].sum()\n",
    "#     expense=x[(x['transaction_type']==1) & (x['time']>lt)]['amount'].sum()\n",
    "#     return income-expense\n",
    "\n",
    "# def afterLoanIncomeChange(x):\n",
    "#     lt=trainLoanTime.loc[x['user_id'].mean(),'time']\n",
    "#     beforeIncome=float(x[(x['transaction_type']==0) & (x['time']<=lt)]['amount'].sum())/float(lt-startTime)\n",
    "#     afterIncome=float(x[(x['transaction_type']==0) & (x['time']>lt)]['amount'].sum())/float(finalTime-lt)\n",
    "#     return afterIncome-beforeIncome\n",
    "\n",
    "# def loanBillFreqChange(x):\n",
    "#     lt=trainLoanTime.loc[x['user_id'].mean(),'time']\n",
    "#     beforeFreq=float(x[x['time']<=lt]['time'].count())/float(lt-startTime)\n",
    "#     afterFreq= float(x[x['time']>lt]['time'].count())/float(finalTime-lt)\n",
    "#     return afterFreq-beforeFreq\n",
    "# def newBanksAfterLoan(x):\n",
    "#     lt=trainLoanTime.loc[x['user_id'].mean(),'time']\n",
    "#     beforeBanks=set(x[x['time']<=lt]['bank_id'].unique())\n",
    "#     afterBanks=set(x[x['time']>lt]['bank_id'].unique())\n",
    "#     return len(afterBanks-beforeBanks)\n",
    "# def terminatedBanksAfterLoan(x):\n",
    "#     lt=trainLoanTime.loc[x['user_id'].mean(),'time']\n",
    "#     beforeBanks=set(x[x['time']<=lt]['bank_id'].unique())\n",
    "#     afterBanks=set(x[x['time']>lt]['bank_id'].unique())\n",
    "#     return len(beforeBanks-afterBanks)\n",
    "\n",
    "\n",
    "def afterLoanBillAdjustment(x):\n",
    "    lt=loanTime.loc[x['user_id'].mean(),'time']\n",
    "    afterLoanX=x[x['time']<=lt]\n",
    "    return (afterLoanX['cur_bill_amount'].mean()-afterLoanX['adjusted_amount'].mean())/(afterLoanX['cur_bill_amount'].mean()+0.00001)\n",
    "\n",
    "def afterLoanUnderpay(x):\n",
    "    lt=loanTime.loc[x['user_id'].mean(),'time']\n",
    "    afterX=x[x['time']>lt]\n",
    "    return (afterX['last_bill_amount']-afterX['last_bill_pay']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add manually picked features to the working dataset\n",
    "def handPickedFeatures(users, banks, browse, bills, loanTimes):\n",
    "\n",
    "    handPickedFeatures=pd.DataFrame(users.index, columns=['id'])\n",
    "    handPickedFeatures.set_index(['id'], inplace=True)\n",
    "\n",
    "\n",
    "    # bank record features\n",
    "    bankRecordsPersonal=banks.groupby('user_id')\n",
    "    handPickedFeatures['transactionNum']=bankRecordsPersonal['amount'].count()\n",
    "    handPickedFeatures['transactionAmount']=bankRecordsPersonal['amount'].sum()\n",
    "    handPickedFeatures['transactionNetAmount']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==1]['amount'].sum())-\\\n",
    "            bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==0]['amount'].sum())\n",
    "    handPickedFeatures['transactionExpense']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==1]['amount'].sum())\n",
    "    handPickedFeatures['transactionExpenseMax']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==1]['amount'].max())\n",
    "    handPickedFeatures['transactionExpenseAvg']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==1]['amount'].mean())\n",
    "    handPickedFeatures['transactionIncome']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==0]['amount'].sum())\n",
    "    handPickedFeatures['transactionIncomeMax']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==0]['amount'].max())\n",
    "    handPickedFeatures['transactionIncomeAvg']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==0]['amount'].mean())\n",
    "    handPickedFeatures['transactionExpenseNum']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==1]['amount'].count())\n",
    "    handPickedFeatures['transactionIncomeNum']=bankRecordsPersonal.apply(lambda x: x[x['transaction_type']==0]['amount'].count())\n",
    "    handPickedFeatures['salaryNum']=bankRecordsPersonal['is_salary'].sum()\n",
    "    handPickedFeatures['salaryTotal']=bankRecordsPersonal.apply(lambda x: x[x['is_salary']==1]['amount'].sum())\n",
    "    handPickedFeatures['salaryAvg']=bankRecordsPersonal.apply(lambda x: x[x['is_salary']==1]['amount'].mean())\n",
    "    handPickedFeatures['salaryMax']=bankRecordsPersonal.apply(lambda x: x[x['is_salary']==1]['amount'].max())\n",
    "\n",
    "\n",
    "    browseRecordsPersonal=browse.groupby('user_id')\n",
    "    handPickedFeatures['browseNum']=browseRecordsPersonal['time'].count()\n",
    "    handPickedFeatures['browseFreq']=(browseRecordsPersonal['time'].count()).div(browseRecordsPersonal['time'].max()-browseRecordsPersonal['time'].min()+1+0.000001)\n",
    "    handPickedFeatures['browseTypes']=browseRecordsPersonal['browse_type'].nunique()\n",
    "    handPickedFeatures['browseSubTypes']=browseRecordsPersonal.apply(lambda x: x.groupby('browse_type')['sub_type'].nunique().sum())\n",
    "\n",
    "\n",
    "    billsPersonal=bills.groupby('user_id')\n",
    "    handPickedFeatures['billBanksNum']=billsPersonal['bank_id'].nunique()\n",
    "    handPickedFeatures['billNum']=billsPersonal['time'].count()\n",
    "    handPickedFeatures['underpay']=billsPersonal.apply(lambda x: 1.0*sum(x['last_bill_amount']-x['last_bill_pay'])/len(x))\n",
    "    handPickedFeatures['avgCreditLine']=billsPersonal['credit_line'].mean()\n",
    "    handPickedFeatures['avgTransactionNum']=billsPersonal['transactionNum'].mean()\n",
    "    handPickedFeatures['maxBankCredit_line']=billsPersonal.apply(lambda x: x.groupby('bank_id')['credit_line'].mean().max())\n",
    "    handPickedFeatures['avgLastBillAmount']=billsPersonal.apply(lambda x: x.groupby('bank_id')['last_bill_amount'].mean().mean())\n",
    "    handPickedFeatures['avgMaxLastBillAmount']=billsPersonal.apply(lambda x: x.groupby('bank_id')['last_bill_amount'].max().mean())\n",
    "    handPickedFeatures['relative_bill_adjustment']=(billsPersonal['cur_bill_amount'].mean()-billsPersonal['adjusted_amount'].mean()).div(billsPersonal['cur_bill_amount'].mean()+0.00001)\n",
    "    handPickedFeatures['avgBillTimeSpan']=billsPersonal['time'].max()-billsPersonal['time'].min()\n",
    "    handPickedFeatures['maxBankBillTimeSpan']=billsPersonal.apply(lambda x: (x.groupby('bank_id')['time'].max()-x.groupby('bank_id')['time'].min()).max())\n",
    "    handPickedFeatures['totalBankBillTimeSpan']=billsPersonal.apply(lambda x: (x.groupby('bank_id')['time'].max()-x.groupby('bank_id')['time'].min()).sum())\n",
    "    handPickedFeatures['debtNum']=billsPersonal['debt_status'].mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    handPickedFeatures['afterLoanBillFreq']=billsPersonal.apply(afterLoanBillFreq)\n",
    "    handPickedFeatures['afterLoanBillNum']=billsPersonal.apply(lambda x: x[x['time']>loanTime.loc[x['user_id'].mean(),'time']]['time'].count())\n",
    "    handPickedFeatures['afterLoanUnderpay']=billsPersonal.apply(afterLoanUnderpay)\n",
    "    handPickedFeatures['afterLoanSalaryChange']=bankRecordsPersonal.apply(afterLoanSalaryChange)\n",
    "    handPickedFeatures['afterLoanBillTimeSpan']=billsPersonal.apply(lambda x: x['time'].max()-loanTime.loc[x['user_id'].mean(),'time'])\n",
    "    handPickedFeatures['beforeLoanBillTimeSpan']=billsPersonal.apply(lambda x: loanTime.loc[x['user_id'].mean(),'time']-x['time'].min())\n",
    "    handPickedFeatures['afterLoanBillAdjustment']=billsPersonal.apply(afterLoanBillAdjustment)\n",
    "\n",
    "\n",
    "    handPickedFeatures['loanTime']=loanTimes['time']\n",
    "\n",
    "    return handPickedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make one-hot encoding for categorical features\n",
    "def convert2Dummies(df, cln):\n",
    "    dummies=pd.get_dummies(df[cln]).astype(int)\n",
    "    dummies.columns=[cln+`c` for c in dummies.columns]\n",
    "\n",
    "    return pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeXYdata(type='train'):\n",
    "    if type=='combine':\n",
    "        users_train, banks_train, browse_train, bills_train, loanTime_train, overdue_train=readFiles(type='train')\n",
    "        users_test, banks_test, browse_test, bills_test, loanTime_test, _=readFiles(type='test')\n",
    "        users=pd.concat([users_train,users_test])\n",
    "        banks=pd.concat([banks_train, banks_test])\n",
    "        browse=pd.concat([browse_train, browse_test])\n",
    "        bills=pd.concat([bills_train, bills_test])\n",
    "        loanTime=pd.concat([loanTime_train, loanTime_test])\n",
    "\n",
    "    else:\n",
    "        users, banks, browse, bills, loanTime, overdue=readFiles(type='train') if type=='train' else readFiles(type='test')\n",
    "\n",
    "    billFeatures=extractBillFeatures(bills, users.index)\n",
    "    browseTypeFeatures=extractBrowseTypeFeatures(browse, users.index)\n",
    "    madeFeatures=handPickedFeatures(users, banks, browse, bills, loanTime)\n",
    "    userFeatures=users.join(billFeatures).join(browseTypeFeatures)\n",
    "    userFeatures=userFeatures.join(madeFeatures)\n",
    "\n",
    "    # create dummies\n",
    "    userFeatures=convert2Dummies(userFeatures, 'sex')\n",
    "    userFeatures=convert2Dummies(userFeatures, 'profession')\n",
    "    userFeatures=convert2Dummies(userFeatures, 'education')\n",
    "    userFeatures=convert2Dummies(userFeatures, 'marriage')\n",
    "    userFeatures=convert2Dummies(userFeatures, 'citizen_type')\n",
    "\n",
    "\n",
    "    # fill in NA with zeros or other numbers\n",
    "    userFeatures['transactionNum'].fillna(0, inplace=True)\n",
    "    userFeatures['transactionNum']=userFeatures['transactionNum'].astype(int)\n",
    "\n",
    "\n",
    "    userFeatures['salaryNum'].fillna(0, inplace=True)\n",
    "    userFeatures['salaryNum']=userFeatures['salaryNum'].astype(int)\n",
    "\n",
    "    # userFeatures['browseNum'].fillna(0, inplace=True)\n",
    "    userFeatures['browseTypes'].fillna(0, inplace=True)\n",
    "    userFeatures['browseTypes']=userFeatures['browseTypes'].astype(int)\n",
    "\n",
    "    userFeatures['browseSubTypes'].fillna(0, inplace=True)\n",
    "    userFeatures['browseSubTypes']=userFeatures['browseSubTypes'].astype(int)\n",
    "\n",
    "    userFeatures['billBanksNum'].fillna(0, inplace=True)\n",
    "    userFeatures['billBanksNum']=userFeatures['billBanksNum'].astype(int)\n",
    "\n",
    "    userFeatures['billNum'].fillna(0, inplace=True)\n",
    "    userFeatures['billNum']=userFeatures['billNum'].astype(int)\n",
    "\n",
    "    userFeatures['salaryTotal'].fillna(0, inplace=True)\n",
    "    userFeatures['transactionAmount'].fillna(0, inplace=True)\n",
    "\n",
    "    print len(userFeatures.columns)\n",
    "\n",
    "    imp=Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "    userFeatures=imp.fit_transform(userFeatures)\n",
    "\n",
    "    print userFeatures.shape\n",
    "\n",
    "    return userFeatures, overdue if type=='train' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produceTrainTest(X_train_, y_train_, X_test=None, y_test=None, test_size=0.2, random_state=8):\n",
    "    if X_test is None:\n",
    "        nTrain=int((1-test_size)*len(X_train_))\n",
    "        X_train=X_train_[:nTrain]\n",
    "        y_train=y_train_[:nTrain]\n",
    "        \n",
    "        X_test=X_train_[nTrain:]\n",
    "        y_test=y_train_[nTrain:]\n",
    "#         X_train, X_test, y_train, y_test=train_test_split(X_train_, y_train_\\\n",
    "#                                                   , test_size=test_size, random_state=random_state)\n",
    "        scaleTemp=X_train_\n",
    "    else:\n",
    "        X_train, y_train=X_train_, y_train_\n",
    "        scaleTemp=np.concatenate([X_train_, X_test])\n",
    "    # standardization\n",
    "    scaler=StandardScaler()\n",
    "    scaler.fit(scaleTemp)\n",
    "    X_train=scaler.transform(X_train)\n",
    "    X_test=scaler.transform(X_test)  \n",
    "    \n",
    "    # compute the correlation between variants and labels\n",
    "    corrmatrix=np.corrcoef(X_train_.T, y_train_)\n",
    "    # compute relative importance of features\n",
    "    \n",
    "    \n",
    "    \n",
    "    factors=abs(np.array(corrmatrix[-1, :-1]))**1.5\n",
    "    for i in xrange(0):\n",
    "        factors=normalize(factors.reshape(1,-1))\n",
    "        factors*=abs(corrmatrix[-1, :-1])\n",
    "    # scale features according to their lative importance\n",
    "    X_train*=factors\n",
    "    X_test*=factors\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeClassifier(X_train, y_train, clf_name):\n",
    "    weights=y_train+0.3\n",
    "    if clf_name=='knn':\n",
    "        # nearest neighbors\n",
    "        return KNeighborsClassifier(n_neighbors=10, weights='distance').fit(X_train, y_train)\n",
    "    elif clf_name=='rf':\n",
    "        # random forest classifier\n",
    "        \n",
    "        return rfc(n_estimators=1200, max_features=8*int(np.sqrt(X_train.shape[1])), max_depth=None, \\\n",
    "                   n_jobs=7, min_samples_split=16, random_state=0).fit(X_train, y_train, sample_weight=weights)\n",
    "    elif clf_name=='xgb':\n",
    "        # xgboost\n",
    "        return XGBClassifier(learning_rate=0.04, n_estimators=1200, subsample=0.9, colsample_bylevel=0.9, \\\n",
    "                         objective='binary:logistic', max_depth=5, gamma=2, seed=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "(55596, 312)\n"
     ]
    }
   ],
   "source": [
    "X, y=makeXYdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55596\n"
     ]
    }
   ],
   "source": [
    "print len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "(69495, 315)\n"
     ]
    }
   ],
   "source": [
    "X_all, _=makeXYdata(type='combine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=X_all[:55596]\n",
    "X_val=X_all[55596:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=produceTrainTest(X, y, X_val)\n",
    "clf=makeClassifier(X_train, y_train, 'xgb')\n",
    "prob=clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit=pd.DataFrame()\n",
    "submit['userid']=np.arange(13899)+55597\n",
    "submit['probability']=prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outFile='../results/submit'+time.ctime()+'.csv'\n",
    "submit.to_csv(outFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "0.133992805755\n",
      "1490\n",
      "5693\n",
      "0.0852348993289\n",
      "Ks_2sampResult(statistic=0.44183793653780484, pvalue=9.9750448265880043e-221)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test=produceTrainTest(X, y)\n",
    "clf=makeClassifier(X_train, y_train, 'rf')\n",
    "prob=clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print len(prob[prob>0.5])\n",
    "print 1.0*sum(y_test)/len(y_test)\n",
    "print sum(y_test)\n",
    "print sum(y_train)\n",
    "print 1.0*len(prob[(prob>0.5)&(y_test==1)])/sum(y_test==1)\n",
    "\n",
    "p=prob[y_test==1]\n",
    "n=prob[y_test==0]\n",
    "print ks_2samp(p,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (13899,37) (305,) (13899,37) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-6e0d5a969068>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproduceTrainTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-d15e58c8808f>\u001b[0m in \u001b[0;36mproduceTrainTest\u001b[1;34m(X_train_, y_train_, X_test, y_test, test_size, random_state)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# compute the correlation between variants and labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zhenjie/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (13899,37) (305,) (13899,37) "
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, _=produceTrainTest(X, y, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-6de5a383494e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproduceTrainTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmakeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xgb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-3414dca575f1>\u001b[0m in \u001b[0;36mproduceTrainTest\u001b[1;34m(X_train_, y_train_, X_test, y_test, test_size, random_state)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# compute the correlation between variants and labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zhenjie/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, copy)\u001b[0m\n\u001b[0;32m    644\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy,\n\u001b[0;32m    645\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zhenjie/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    380\u001b[0m                                       force_all_finite)\n\u001b[0;32m    381\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "clf=makeClassifier(X_train, y_train, 'xgb')\n",
    "prob=clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55596\n"
     ]
    }
   ],
   "source": [
    "print len(X_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
